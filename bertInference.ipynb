{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e060890a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Fawaz\\Desktop\\AI_Text_Detector\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bcd305e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "CUDA is not available, using CPU\n"
     ]
    }
   ],
   "source": [
    "# Set device (GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Check CUDA availability\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available\")\n",
    "    print(f\"Device count: {torch.cuda.device_count()}\")\n",
    "    print(f\"Current device name: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "else:\n",
    "    print(\"CUDA is not available, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c05eb16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded from: C:\\Users\\Fawaz\\Desktop\\AI_Text_Detector\\human-ai-model\n",
      "Model loaded from: C:\\Users\\Fawaz\\Desktop\\AI_Text_Detector\\human-ai-model\n",
      "Model moved to device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load the fine-tuned model and tokenizer\n",
    "model_path = r\"C:\\Users\\Fawaz\\Desktop\\AI_Text_Detector\\human-ai-model\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "print(f\"Tokenizer loaded from: {model_path}\")\n",
    "\n",
    "# Load model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print(f\"Model loaded from: {model_path}\")\n",
    "print(f\"Model moved to device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f110b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prediction function\n",
    "def predict_text_source(text: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Predict whether a given text is Human or AI-generated.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to classify\n",
    "        model: The loaded DistilBERT model\n",
    "        tokenizer: The loaded tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        str: Prediction result with confidence score\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "    \n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = softmax(outputs.logits, dim=1).squeeze()\n",
    "        pred = torch.argmax(probs).item()\n",
    "    \n",
    "    # Convert prediction to label\n",
    "    label = \"Human\" if pred == 0 else \"AI\"\n",
    "    confidence = probs[pred].item()\n",
    "    \n",
    "    return f\"{label} (Confidence: {confidence:.3f})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd048147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model with example texts:\n",
      "============================================================\n",
      "\n",
      "1. Text: This is a simple human-written text about everyday life.\n",
      "   Prediction: AI (Confidence: 1.000)\n",
      "\n",
      "2. Text: The artificial intelligence system generated this response using advanced language models.\n",
      "   Prediction: AI (Confidence: 1.000)\n",
      "\n",
      "3. Text: I went to the store yesterday and bought some groceries.\n",
      "   Prediction: AI (Confidence: 0.998)\n",
      "\n",
      "4. Text: Based on the provided context, I can analyze the data and provide insights.\n",
      "   Prediction: AI (Confidence: 1.000)\n",
      "\n",
      "5. Text: The weather is really nice today, I think I'll go for a walk.\n",
      "   Prediction: AI (Confidence: 0.997)\n"
     ]
    }
   ],
   "source": [
    "# Test the model with some example texts\n",
    "test_texts = [\n",
    "    \"This is a simple human-written text about everyday life.\",\n",
    "    \"The artificial intelligence system generated this response using advanced language models.\",\n",
    "    \"I went to the store yesterday and bought some groceries.\",\n",
    "    \"Based on the provided context, I can analyze the data and provide insights.\",\n",
    "    \"The weather is really nice today, I think I'll go for a walk.\"\n",
    "]\n",
    "\n",
    "print(\"Testing the model with example texts:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for i, text in enumerate(test_texts, 1):\n",
    "    prediction = predict_text_source(text, model, tokenizer)\n",
    "    print(f\"\\n{i}. Text: {text}\")\n",
    "    print(f\"   Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347a035c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "INTERACTIVE PREDICTION MODE\n",
      "============================================================\n",
      "Enter text to classify (or 'quit' to exit):\n"
     ]
    }
   ],
   "source": [
    "# Interactive prediction function\n",
    "def interactive_prediction():\n",
    "    \"\"\"\n",
    "    Interactive function to test the model with user input.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"INTERACTIVE PREDICTION MODE\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"Enter text to classify (or 'quit' to exit):\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nEnter text: \")\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"Exiting...\")\n",
    "            break\n",
    "        \n",
    "        if user_input.strip():\n",
    "            prediction = predict_text_source(user_input, model, tokenizer)\n",
    "            print(f\"Prediction: {prediction}\")\n",
    "        else:\n",
    "            print(\"Please enter some text.\")\n",
    "\n",
    "# Uncomment the line below to run interactive mode\n",
    "interactive_prediction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d9987e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get detailed prediction probabilities\n",
    "def get_detailed_prediction(text: str, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Get detailed prediction probabilities for both Human and AI classes.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text to classify\n",
    "        model: The loaded DistilBERT model\n",
    "        tokenizer: The loaded tokenizer\n",
    "    \n",
    "    Returns:\n",
    "        dict: Detailed prediction results\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors=\"pt\", \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probs = softmax(outputs.logits, dim=1).squeeze()\n",
    "        \n",
    "    return {\n",
    "        \"human_probability\": probs[0].item(),\n",
    "        \"ai_probability\": probs[1].item(),\n",
    "        \"prediction\": \"Human\" if probs[0] > probs[1] else \"AI\",\n",
    "        \"confidence\": max(probs[0].item(), probs[1].item())\n",
    "    }\n",
    "\n",
    "# Test detailed prediction\n",
    "example_text = \"This is a test text to demonstrate the detailed prediction functionality.\"\n",
    "detailed_result = get_detailed_prediction(example_text, model, tokenizer)\n",
    "\n",
    "print(\"Detailed Prediction Results:\")\n",
    "print(f\"Text: {example_text}\")\n",
    "print(f\"Human Probability: {detailed_result['human_probability']:.3f}\")\n",
    "print(f\"AI Probability: {detailed_result['ai_probability']:.3f}\")\n",
    "print(f\"Prediction: {detailed_result['prediction']}\")\n",
    "print(f\"Confidence: {detailed_result['confidence']:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
